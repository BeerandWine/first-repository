{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736c95e5",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence(seq2seq)\n",
    "\n",
    "#### image-to-text, voice-to-text 등, 딥러닝을 통해 어떤 복잡한 것도 다른 것으로 바꿀 수 있다는 X-to-Y 모델에 대한 아이디어\n",
    "\n",
    "다양한 RNN의 구성을 알아보기\n",
    "\n",
    "인코더와 디코더 구조의 필요성 이해하기\n",
    "\n",
    "교사 강요(teather forcing)의 원리 알기\n",
    "\n",
    "훈련 단계와 추론 단계(inference)의 차이 알기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1bb7bd",
   "metadata": {},
   "source": [
    "Rule based Machine Translation언어학 기반 기계 번역 - Statistical Machine Translation통계적 기계 번역 - Neural Machine Translation신경망 기계 번역\n",
    "\n",
    "문장을 끝까지 인식한 후에 번역이나 통역을 해야 하기 때문에 입력 즉시 출력을 하는 것이 아니라 특정 time step에서 출력이 나오는 구조적 특징(many to many)\n",
    "\n",
    "#### seq2seq_두 개의 RNN 아키텍처를 연결한 구조\n",
    "\n",
    "입력문장을 받은 RNN인코더는 데이터 X를 해석하기 위한 저차원 feature vector z를 만들어내고, RNN디코더는 저차원의 feature z로부터 정보를 복원해서 다시 어떤 데이터 X'를 재생성.\n",
    "\n",
    "feature vector z는 인코더RNN이 만들어낸 hidden state 벡터에 해당하고 디코더RNN에 전달하게 된다.(그렇기 때문에 인코더의 마지막 time step의 hidden state를 필요로 한다)\n",
    "\n",
    "## Conditional Language Model\n",
    "\n",
    "Text Generator문장 생성기는 Language Model언어모델을 구현한 것인데, 확률 모델의 특성상 어떤 단어가 생성될지 알 수 없다. 조건 C에 적합하도록 제어할 수 있다면 아주 유용한데, \n",
    "\n",
    "#### '프랑스어문장 Y를 생성하되, 해당 문장은 영어로 X라는 의미여야 해'에 해당. 문장X를 해석해서 C로 만드는 RNN인코더가 존재하는 형태.\n",
    "\n",
    "## teacher forcing교사 강요\n",
    "\n",
    "seq2seq는 훈련과정과 추론과정에서의 동작 방식이 다르다. 디코더RNN은 이전 time step의 출력을 현재 time step의 입력으로 사용한다.(추론시 동작방식)\n",
    "\n",
    "훈련 과정에서 이전 time step이 잘못된 예측을 한다면, 이를 입력으로한 현재 time step의 예측도 잘못될 수 있고 이런 상황이 반복되면 훈련시간이 늘어나게된다. 훈련 과정에서는 정답 시퀀스를 알고 있기 때문에( 이전 time step의 예측값을 현재 time step의 입력으로 사용하지 않는다.)이전 time step의 실제값을 사용한다.(훈련데이터에 과대적합할 수 있음)\n",
    "\n",
    "## 단어 수준의 번역과 문자 수준의 번역은 어느 쪽이 더 유리한가?\n",
    "\n",
    "서로 간에 장단점이 있고, trade-off관계이기도 하다. 각 언어의 특성을 고려했을 때 전처리의 문제(비슷한 단어, 띄어쓰기...)를 쉽게 해결하려면 문자 수준으로 번역하면 되겠지만, 이 과정에서 단어 안에 내재된 정보가 소실될 수 있다. 문자가 단어를 이루는 패턴까지 학습해야 할 필요가 발생하기 때문에, 충분한 데이터가 확보되지 않았다면 문자 수준의 번역이 단어 수준의 번역보다 품질이 떨어진다.\n",
    "\n",
    "#### 문자 수준의 번역기를 만들어보면, 단어 수준의 번역기는 쉽게 만들 수 있을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4af094",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리\n",
    "\n",
    "1-0 패키지 호출\n",
    "\n",
    "1-1 해당파일을 데이터프레임으로 출력\n",
    "\n",
    "1-2 불필요한 열을 제거, 훈련 데이터로 5000개만 이용하도록 설정\n",
    "\n",
    "1-3 (seq2seq 동작을 위해) 디코더의 입력과 예측에 시작 토큰('\\t')과 종료 토큰('\\n')을 삽입\n",
    "\n",
    "1-4 각 언어의 단어장 생성, (각 단어에 부여된 고유한 정수로 )텍스트 시퀀스를 정수 시퀀스로 인코딩\n",
    "\n",
    "1-5 단어장 크기를 변수로 저장(0번 토큰을 고려하여 +1)\n",
    "\n",
    "1-6 각 언어 데이터의 최대 길이를 확인\n",
    "\n",
    "1-7 프랑스어 시퀀스의 경우, ①교사 강요를 위해 디코더의 입력으로 사용할 시퀀스(eos 토큰 필요없음), ②디코더의 출력과 비교할 정답 시퀀스(sos 토큰 필요없음)가 필요.\n",
    "\n",
    "1-8 확인했던 각 언어 데이터의 최대 길이를 이용해서 패딩을 진행\n",
    "\n",
    "1-9 각 정수에 대해 원-핫 인코딩 벡터화\n",
    "\n",
    "1-10 데이터 50000건 3000건을 검증데이터로, 나머지를 학습데이터로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7d7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 호출\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54fe4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 217975\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183209</th>\n",
       "      <td>I have a very important decision to make.</td>\n",
       "      <td>Je dois prendre une décision très importante.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133798</th>\n",
       "      <td>What did you do on the weekend?</td>\n",
       "      <td>Qu'as-tu fait le week-end ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209713</th>\n",
       "      <td>Some people think the government has way too m...</td>\n",
       "      <td>Certaines personnes pensent que le gouvernemen...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109675</th>\n",
       "      <td>We came so close to winning.</td>\n",
       "      <td>On est passés si près de la victoire.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179431</th>\n",
       "      <td>I don't think that we can do that today.</td>\n",
       "      <td>Je ne pense pas que nous puissions le faire au...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "183209          I have a very important decision to make.   \n",
       "133798                    What did you do on the weekend?   \n",
       "209713  Some people think the government has way too m...   \n",
       "109675                       We came so close to winning.   \n",
       "179431           I don't think that we can do that today.   \n",
       "\n",
       "                                                      fra  \\\n",
       "183209      Je dois prendre une décision très importante.   \n",
       "133798                        Qu'as-tu fait le week-end ?   \n",
       "209713  Certaines personnes pensent que le gouvernemen...   \n",
       "109675              On est passés si près de la victoire.   \n",
       "179431  Je ne pense pas que nous puissions le faire au...   \n",
       "\n",
       "                                                       cc  \n",
       "183209  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "133798  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "209713  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "109675  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "179431  CC-BY 2.0 (France) Attribution: tatoeba.org #7...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 해당 파일을 데이터프레임으로 읽어온다\n",
    "\n",
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) # 샘플을 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7760ff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47868</th>\n",
       "      <td>I'm friends with Tom.</td>\n",
       "      <td>Tom et moi sommes amis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33476</th>\n",
       "      <td>I'm supposed to go.</td>\n",
       "      <td>Je suis censé partir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10809</th>\n",
       "      <td>I am off today.</td>\n",
       "      <td>Je suis en congé, aujourd'hui.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>Now I understand.</td>\n",
       "      <td>Maintenant je comprends.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17468</th>\n",
       "      <td>Tom scares easy.</td>\n",
       "      <td>Tom a la trouille facile.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         eng                             fra\n",
       "47868  I'm friends with Tom.         Tom et moi sommes amis.\n",
       "33476    I'm supposed to go.           Je suis censé partir.\n",
       "10809        I am off today.  Je suis en congé, aujourd'hui.\n",
       "21612      Now I understand.        Maintenant je comprends.\n",
       "17468       Tom scares easy.       Tom a la trouille facile."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불필요한 3번째 열을 제거, 샘플의 수를 50000개로 설정\n",
    "\n",
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fffb9bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3204</th>\n",
       "      <td>I got fined.</td>\n",
       "      <td>\\t On m'a collé une amende. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36079</th>\n",
       "      <td>We don't need that.</td>\n",
       "      <td>\\t Nous n'avons pas besoin de ça. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42542</th>\n",
       "      <td>Tom collects comics.</td>\n",
       "      <td>\\t Tom collectionne les comics. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>Good morning.</td>\n",
       "      <td>\\t Bonjour. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13935</th>\n",
       "      <td>Are you smiling?</td>\n",
       "      <td>\\t Êtes-vous en train de sourire ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                    fra\n",
       "3204           I got fined.         \\t On m'a collé une amende. \\n\n",
       "36079   We don't need that.   \\t Nous n'avons pas besoin de ça. \\n\n",
       "42542  Tom collects comics.     \\t Tom collectionne les comics. \\n\n",
       "4850          Good morning.                         \\t Bonjour. \\n\n",
       "13935      Are you smiling?  \\t Êtes-vous en train de sourire ? \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더의 입력과 예측에 시작 토큰, 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb571f4",
   "metadata": {},
   "source": [
    "영어와 프랑스어 별도의 단어장을 만들고, 단어에 부여된 고유한 정수로 텍스트 시퀀스를 정수 시퀀스로 변환하는 정수 인코딩 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e567e479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 4, 7], [19, 4, 7], [19, 4, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 단어장 만들기\n",
    "\n",
    "eng_tokenizer = Tokenizer(char_level=True) # 문자 단위로 Tokenizer 생성\n",
    "eng_tokenizer.fit_on_texts(lines.eng)       # eng 50000개의 행에 대해 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng) # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ff5a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 19, 5, 1, 31, 1, 11],\n",
       " [10, 1, 15, 5, 12, 16, 29, 2, 14, 1, 11],\n",
       " [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프랑스어 단어장 만들기\n",
    "\n",
    "fra_tokenizer = Tokenizer(char_level=True) # 문자 단위로 토큰화\n",
    "fra_tokenizer.fit_on_texts(lines.fra)       # fra 50000개의 행에 대해 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra) # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a20580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 52\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "# 단어장의 크기를 변수로 저장. 0번 토큰을 고려하여 +1\n",
    "\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15db60be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 21\n",
      "프랑스어 시퀀스의 최대 길이 69\n"
     ]
    }
   ],
   "source": [
    "# padding 을 위해 최대 길이를 확인\n",
    "# 모델에 입력될 영어, 프랑스어 시퀀스의 길이가 일정해야 하기 때문에 최대길이를 맞추고 시퀀스 뒷부분은 패딩으로 채운다.\n",
    "\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62446255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 52\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 21\n",
      "프랑스어 시퀀스의 최대 길이 69\n"
     ]
    }
   ],
   "source": [
    "# 전체 통계 정보 출력\n",
    "\n",
    "print('전체 샘플의 수 :', len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ba265",
   "metadata": {},
   "source": [
    "영어 시퀀스는 인코더의 입력으로 사용된다. 프랑스어 시퀀스의 경우는\n",
    "\n",
    "#### 디코더의 출력과 비교할 정답 데이터(원래 목적)로 사용할 / Teacher forcing교사 강요를 위해 디코더의 입력으로 사용할\n",
    "\n",
    "2가지 버전이 필요하다.\n",
    "\n",
    "#### 디코더의 출력과 비교할 시퀀스는 sos토큰이 필요없고 / 디코더의 입력으로 사용할 시퀀스는 eos토큰이 필요없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b02427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text]\n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e0afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 1, 19, 5, 1, 31, 1], [10, 1, 15, 5, 12, 16, 29, 2, 14, 1], [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1]]\n",
      "[[1, 19, 5, 1, 31, 1, 11], [1, 15, 5, 12, 16, 29, 2, 14, 1, 11], [1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "# decoder의 입력과 출력을 각각 출력\n",
    "\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])\n",
    "\n",
    "# decoder의 입력은 11(eos토큰)이 제거되었고,\n",
    "# decoder의 출력은 10(sos토큰)이 제거되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e64281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69)\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "# decoder의 입력과 출력 시퀀스를 토큰 편집한 후, 모든 샘플들의 길이를 동일하게 변환, 시퀀스 뒷부분은 패딩으로 채운다.\n",
    "# 영어 데이터의 모든 샘플들은 max_eng_seq_len의 길이, 프랑스어의 모든 샘플들은 max_fra_seq_len의 길이로 맞춘다.\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db51a773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  4  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# 인코더 샘플 출력\n",
    "\n",
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeab7d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 21, 52)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 69, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 69, 73)\n"
     ]
    }
   ],
   "source": [
    "# 각 정수에 대해 원-핫 인코딩으로 벡터화\n",
    "\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))\n",
    "\n",
    "# 원-핫 인코딩은 각 정수를 단어장의 크기를 가지는 원-핫 벡터로 인코딩하기 때문에\n",
    "# 원-핫 인코딩을 하고나서의 데이터의 크기는 (샘플의 수 x 샘플의 길이 x 단어장의 크기) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90283bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 21, 52)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 69, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 69, 73)\n"
     ]
    }
   ],
   "source": [
    "# 50000건 중 3000건은 검증데이터, 나머지는 학습데이터\n",
    "\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722a72d",
   "metadata": {},
   "source": [
    "## 2. Modeling_seq2seq\n",
    "\n",
    "2-0 패키지 호출\n",
    "\n",
    "2-1 encoder 설계(인코더 LSTM 셀의 마지막 time state의 hidden state와 cell state를, 디코더LSTM의 첫번째 hidden state와 cell state로 전달)\n",
    "\n",
    "2-2 decoder 설계(인코더의 마지막 hidden state와 cell state를 저장해 두었다가 initial_state를 이용해서, LSTM셀의 초기 상태를 정의\n",
    "\n",
    "2-3 decoder의 출력층을 설계(프랑스어 단어장의 크기를 기재, 활성화 함수로 소프트맥스를 사용)\n",
    "\n",
    "2-4 compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "2-5 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f65db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 호출\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e27f01",
   "metadata": {},
   "source": [
    "일반적인 RNN의 경우에는, 인코더의 마지막 hidden state를 디코더의 첫번째 hidden state로 사용한다.\n",
    "\n",
    "#### 인코더LSTM 셀의 마지막 time step의 hidden state와 cell state를 디코더LSTM의 첫번째 hidden state와 cell state로 전달해주어야 한다.\n",
    "\n",
    "(전달하는 내용을 '리턴 시퀀스=트루, 리턴 스테이트=트루'로 표현한듯 하다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e3c8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 텐서 생성\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "\n",
    "# hidden size가 256인 인코더 LSTM셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_output은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49d8745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "\n",
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "\n",
    "# hidden size가 256인 인코더의 LSTM셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "\n",
    "# decoder_outputs은 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68289e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더의 출력층 설계\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ba27db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 52)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 316416      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 673,097\n",
      "Trainable params: 673,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b46cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 43s 19ms/step - loss: 0.9619 - val_loss: 0.8191\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.5864 - val_loss: 0.6839\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.4919 - val_loss: 0.5936\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.4295 - val_loss: 0.5378\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.3899 - val_loss: 0.5092\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.3630 - val_loss: 0.4769\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.3407 - val_loss: 0.4554\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.3232 - val_loss: 0.4335\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.3088 - val_loss: 0.4262\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2966 - val_loss: 0.4179\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2861 - val_loss: 0.4051\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2775 - val_loss: 0.3986\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2693 - val_loss: 0.3929\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2616 - val_loss: 0.3868\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2549 - val_loss: 0.3857\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2486 - val_loss: 0.3809\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2429 - val_loss: 0.3814\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2377 - val_loss: 0.3771\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2329 - val_loss: 0.3780\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.2282 - val_loss: 0.3767\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2239 - val_loss: 0.3752\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2196 - val_loss: 0.3748\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2159 - val_loss: 0.3768\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2122 - val_loss: 0.3728\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2086 - val_loss: 0.3753\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2051 - val_loss: 0.3760\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.2020 - val_loss: 0.3741\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1988 - val_loss: 0.3779\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1959 - val_loss: 0.3775\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1929 - val_loss: 0.3782\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1902 - val_loss: 0.3787\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1874 - val_loss: 0.3817\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1849 - val_loss: 0.3839\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1823 - val_loss: 0.3868\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1799 - val_loss: 0.3879\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1774 - val_loss: 0.3921\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1751 - val_loss: 0.3936\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1730 - val_loss: 0.3934\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1707 - val_loss: 0.3952\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1686 - val_loss: 0.3959\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 6s 18ms/step - loss: 0.1666 - val_loss: 0.3993\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1646 - val_loss: 0.4018\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1627 - val_loss: 0.4002\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1607 - val_loss: 0.4055\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1589 - val_loss: 0.4050\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1571 - val_loss: 0.4120\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1553 - val_loss: 0.4119\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1537 - val_loss: 0.4134\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1520 - val_loss: 0.4144\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.1504 - val_loss: 0.4156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f145aa541f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "         validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "         batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3288bd",
   "metadata": {},
   "source": [
    "## 3. Testing Model\n",
    "\n",
    "3-1 encoder의 정의\n",
    "\n",
    "3-2 decoder를 설계\n",
    "\n",
    "3-3 decoder의 출력층을 재설계\n",
    "\n",
    "3-4 단어에서 정수로, 정수에서 단어로 바꾸는 사전을 준비\n",
    "\n",
    "3-5 예측 과정을 위한 함수를 구현(번역하고자 하는 문장의 정수 시퀀스를 입력하면, 내부의 인코더를 통해 마지막 시점의 hidden state를 리턴)\n",
    "\n",
    "3-6 테스트\n",
    "\n",
    "seq2seq는 훈련시와 테스트시 동작이 다르기 때문에, 테스트 단계의 디코더 모델은 설계를 다시 해줄 필요가 있다.\n",
    "\n",
    "#### 테스트 단계에서의 디코더 동작 순서\n",
    "\n",
    "#### 1. 인코더에 입력 문장을 넣어 마지막 time step의 hidden, cell state를 얻는다.\n",
    "\n",
    "#### 2. sos 토큰인 '\\t'를 디코더에 입력한다.\n",
    "\n",
    "#### 3. 이전 time step의 출력층의 예측 결과를 현재 time step의 입력으로 한다.\n",
    "\n",
    "#### 4. 3을 반복하다가 eos토큰인 '\\n'이 예측되면 이를 중단한다.\n",
    "\n",
    "이전 time step의 출력층의 예측 결과를 현재 time step의 입력으로 사용하는 단계를 추가해야 하기 때문에, 루프를 돌며 디코더의 LSTM셀을 수동으로 제어하는 느낌의 설계를 해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "996f207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 52)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 316416    \n",
      "=================================================================\n",
      "Total params: 316,416\n",
      "Trainable params: 316,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder를 정의(encoder_inputs와 encoder_states는 이미 정의한 것들을 재사용)\n",
    "\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ff0f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더를 설계\n",
    "\n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "\n",
    "# 이전 time step의 cell state를 저정하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73c814e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층을 재설계\n",
    "\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdbaee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에서 정수로, 정수에서 단어로 바꾸는 사전을 준비\n",
    "# 문장을 숫자 인덱스로 바꾸는 Tokenizer를 만드는 과정에서 자동으로 만들어진 사전을 이미 갖고 있다.\n",
    "\n",
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009325c",
   "metadata": {},
   "source": [
    "decode_sequence()의 입력으로 들어가는 것은 번역하고자 하는 문장의 정수 시퀀스.\n",
    "\n",
    "decode sequence()내부에는 인코더를 구현한 encoder_model이 있어서 이 모델에 번역하고자 하는 문장의 정수 시퀀스인 'input_seq'를 입력하면, encoder_model은 마지막 시점의 hidden state를 리턴한다.\n",
    "\n",
    "hidden state는 디코더의 첫번째 시점의 hidden state가 되고, 디코더는 번역 문장을 완성하는 예측 과정을 진행.\n",
    "\n",
    "디코더의 예측 과정은 이전 시점의 예측한 단어를 디코더의 현재 시점의 입력으로 넣어주는 작업을 진행.\n",
    "\n",
    "이 작업은 종료 토큰을 만나거나 주어진 최대 길이를 넘을 때까지 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b80e314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 과정을 위한 함수 decode_sequence()를 구현\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # <sos>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이전 시점의 상태 state_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "        \n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "            \n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        state_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4c7223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Go.\n",
      "정답 문장:  Bouge ! \n",
      "번역기가 번역한 문장:                                                                      \n",
      "-----------------------------------\n",
      "입력 문장: Hello!\n",
      "정답 문장:  Bonjour ! \n",
      "번역기가 번역한 문장:                                                                      \n",
      "-----------------------------------\n",
      "입력 문장: Got it!\n",
      "정답 문장:  Compris ! \n",
      "번역기가 번역한 문장:                                                                      \n",
      "-----------------------------------\n",
      "입력 문장: Goodbye.\n",
      "정답 문장:  Au revoir. \n",
      "번역기가 번역한 문장:                                                                      \n",
      "-----------------------------------\n",
      "입력 문장: Hands off.\n",
      "정답 문장:  Pas touche ! \n",
      "번역기가 번역한 문장:                                                                      \n"
     ]
    }
   ],
   "source": [
    "# 문장의 인덱스를 임의로 입력해서 출력 결과를 테스트\n",
    "\n",
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 결과를 확인하고 싶은 문장의 인덱스(임의로 선정)\n",
    "    input_seq = encoder_input[seq_index: seq_index +1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedc3ad",
   "metadata": {},
   "source": [
    "# PROJECT_단어 Level로 번역기 업그레이드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fda36f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "# 주요 라이브러리의 버전 확인\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f800f1",
   "metadata": {},
   "source": [
    "동일한 데이터셋을 사용하되\n",
    "\n",
    "#### ①글자 단위와는 다른 전처리,\n",
    "\n",
    "#### ②to_categorical()함수가 아닌 Embedding layer임베딩 층을 추가하여\n",
    "\n",
    "단어 단위의 번역기를 완성. 단어 단위로 번역기 진행시 글자 단위에 비해 단어장의 크기가 커지고, 학습 속도가 느려지기 때문에 원활한 진행을 위해 상위 33,000개의 샘플 중 3,000개는 테스트 데이터로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017612b0",
   "metadata": {},
   "source": [
    "### step1. 정제/정규화/전처리\n",
    "\n",
    "#### Puntuation구두점의 분리\n",
    "\n",
    "#### 소문자로 변환\n",
    "\n",
    "#### 띄어쓰기 단위로 토큰화\n",
    "\n",
    "### step2. 디코더의 문장에 시작 토큰과 종료 토큰 삽입\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f077504",
   "metadata": {},
   "source": [
    "#### 7-7 샘플을 뽑아낼 때, '랜덤하게'뽑아내는 코드가 도대체 어디에 있는 것인가?;; sample( ) 함수는 'random'모듈에 속한 함수 중 하나로, 시퀀스에서 중복되지 않는 임의로 요소를 선택하여 반환하는 기능\n",
    "\n",
    "#### EOS와 SOS를 없애는 시점과 이유에 대한 정리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
